# 一些训练的东西

## MSE & RMSE

MES ： 均方误差

  $MSE = \frac{1}{n} \sum^n_{i=1}(y_i-\hat{y}_i)^2$

RMSE： 均方根误差

  $RMSE = \sqrt{\frac{1}{n}\sum^n_{i=1}(y_i-\hat{y}_i)^2}$

均方误差通常用于回归问题，预测的是连续的数值，需要衡量预测值与其真实值之间的距离或者误差

对于分类问题来说，通常使用：

  * acc
  * precision：在所有被预测为正类的样本中，真正为正类的比例

    $Precision = \frac{TP}{TP+FP}$
    
  * recall:在所有真实为正类的样本中，被正确预测为正类的比例
    
      $Recall = \frac{TP}{TP+FN}$
    
  * f1分数：精确率和召回率的调和平均数
    
      $F1 = 2\cdot \frac{Precision \cdot Recall}{Precision + Recall}$
    
* 对数损失 LogLoss:衡量概率预测的不确定性

  $LogLoss = - 1\frac{1}{N}\sum^N_{i=1}[y_ilog(p_i)+(1-y_i)log(1-p_i)]$

* 交叉熵损失，预测概率分布与真实分布之间的差异，多分类问题公式：

    $CrossEntropy = -\sum^N_{i=1}\sum^M_{j=1}y_{ij}log(p_{ij})$
  
关于为什么对数损失和交叉熵损失使用负数：

  在信息伦中，当一个事件的概率为 $p$时，该事件包含的信息量定义为 $-log(p)$，概率越小，信息量越大，即罕见事件发生时提供的信息更多

  在机器学习中，我们希望通过最大化模型对正确标签的预测概率，即最大化对数似然： $最大化 log(p(正确类别))$

  最小化损失函数：但是由于优化算法通常设计为最小化目标函数，所以我们取负值，将最大化问题转换为最小化问题，$最小化:-log(p(正确类别))$

  梯度特性：负对数函数在概率接近1时梯度接近0，在概率接近0时梯度非常大
  
  * 当模型对正确类别非常自信( $p \approx 1$)时，损失很小，梯度也很小

  * 当模型对正确类别完全不自信( $p \approx 0$)时，损失无限大，梯度也很小
  
  *TP*：实际为正，预测为正
  
  *TN*：实际为负，预测为负
  
  *FP*：实际为负，预测为正
  
  *FN*：实际为正，预测为负

**为什么要使用最大似然估计**

在统计轮中，在一定条件下，最大似然估计量是一致的、渐近有效和渐近正态的，这意味着随着样本量增加，估计会收敛到真实参数值

1. 一致性： 意味着随着样本量 $n$趋于无穷大，估计值会概率收敛到真实参数值
2. 渐近有效：在所有无偏估计中，渐近有效的估计量达到克拉美-拉奥下界(方差的理论下限)；简单来说：在足够多的样本下，没有其他估计方法能比它更佳精确，具有更小的方差
3. 渐近正态：当样本足够大时，估计量的分布(参数模型的估计值)近似于正态分布；大样本下，估计误差遵循正态分布，使得可以构建置信区间和进行假设检验；参数的正态分布具有完整的理论支持，比如可以构建置信区间和进行假设检验，评估参数估计的不确定性；并且有很多明确的数学特性，可以使得我们更可靠地量化模型的不确定性，可以将复杂模型的数学分析更加简单

**为什么分类任务使用交叉熵而不是MSE**

1. 梯度消失问题： 当使用sigmoid或者softmax激活函数的时候，如果使用MSE，当预测值与真实值相差巨大会导致梯度变得很小，使得模型训练缓慢；但是交叉熵可以避免这个问题
2. 概率解释：在分类任务中，模型输出可以解释为个类别的概率分布，而交叉熵正好可以度量真实分布于预测分布之间的差异
3. 更快地收敛：对于分类来说，使用交叉熵收敛更好
4. 最大似然估计：

     从统计学角度来看，最小化交叉熵等价于最大化似然函数，这和分类问题的目标一样

     假设使用概率分布问题 $p(y|x)$来建模分类问题，其中 $x$是输入特征， $y$是类别标签，从统计学角度来看，我们希望找到模型参数 $\theta$使得观测数据的似然度最大：对数似然函数为：

     $log L(\theta) = \sum_i log p(y_i|x_i;\theta)$

   对于二分类问题，假设 $p(y=1|x;\theta) = \hat{y}$和 $p(y=0|x;\theta) = 1-\hat{y}$，那么对数似然可以写成：

     $log L(\theta) = \sum^n_{i=1}[y_ilog\hat{y}_i+(1-y_i)log(1-\hat{y}_i)]$

   最大化对数似然可以等价于最小化负对数似然：

     $-log L(\theta) = \sum^n_{i=1}[y_ilog\hat{y}_i-(1-y_i)log(1-\hat{y}_i)]$

   因此，对于多分类问题，可以类似推导为：

     $-log L(\theta) = \sum^n_{i=1}\sum^k_{j=1}-y_{ij}log\hat{y}_{ij}$

   从最大似然俩看，MSE相当于假设数据服从高斯分布的情况下的负对数似然，但是分类问题是离散的，可以使用概率分布，不符合高斯分布假设

6. 处理不平衡数据： 交叉熵在处理类别不平衡的数据集时通常比MSE更有效
7. 信息论： 从信息论来说，交叉上度量的是使用预测分布 $q$编码与真实分布 $p$所需的平均比特数，这和分类任务的本质高度一致

