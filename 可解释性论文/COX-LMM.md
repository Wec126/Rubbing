# CoX-LMM -> 用于解释大型多模态模型的内部表示[https://arxiv.org/html/2406.08074v3]
借鉴CNN领域中的概念激活向量方法，开发专门适用于大型多模态模型的可解释性技术

**Cox-LMM**:
1. 为特定词汇/标记学习概念词典
2. 将模型内部表示通过字典学习方法进行线性分解
3. 分解得到的字典元素代表不同的概念

**关键发现**：
学习到的概念具有多模态语义结构，可以同时在视觉和文本领域中有意义地解释：
* 视觉上：通过提取最大激活这些概念的图像来体现
* 文本上：通过语言模型解码概念病提取相关词汇/标记来表达

**主要贡献**
1. 提出首个针对大规模多模态模型的概念解释框架
2. 开发**基于字典学习的概念提取方法**，使得概念可在文本和视觉上**同时解释**
3. 通过半非负矩阵分解优化改进了现有的概念字典学习策略
4. 通过定性可视化和定量评估验证了多模态概念的有效性

**Related-work**
1. 尽管视觉和感知标记不同，但是他们在大型语言模型内部被隐识对齐
2. ‘多模态神经元’：这些神经元对输入图像中的特定概念信息会被激活
3. 使用CLIP的视觉表示分解为文本表示来理解其内部机制
4. LMM中存在多模态神经元，这些神经元能将高级视觉信息转换为相应的文本信息

## Approach
### 1. Background for Large Multimodal Models
**Model Architecture**
考虑一个大型多模态模型 $f$ 的通用模型架构，他由一个可视编码器 $f_V$、一个可训练连接器 $C$，以及一个包含 $N_L$层的LLM $f_{LM}$。

当前模型针对图像描述任务进行了训练(captioning)，底层数据集 $S=\lbrace (X_i,y_i) \rbrace^N_{i=1}$ 由图像组成，其中图像 $X_i \in \mathcal{X}$并且与他们相关的标题 $y_i \in \mathcal{y}$,其中 $\mathcal{X}$和 $\mathcal{y}$分别表示图像空间和文本标记集。caption $y_i$可以被看作是所有tokens的子集，语言模型 $f_{LM}$的输入是有token $\lbrace h^1,h^2,...,h^p \rbrace$的序列表示，输出为 $\hat{y}$.

token在 $f_{LM}$的第 $l$层位置 $p$处的token表示为 $h^p_{(l)}$，其中 $h^p_{(0)}=h^p$表示位置 $p$处的初始输入表示

多模态模型的输入序列结构和预测过程如下：
多模态模型的语言模型 $f_{LM}$输入序列由两部分组成：
1. 视觉编码器 $f_V$处理图像 $X$后提供的 $N_V$个视觉标记，后接连接器 $C$
2. 语言模型之间预测的文本标记的线性嵌入


对于位置 $p > N_V$的情况，也就是非视觉标记的部分，预测过程表示为：

$\hat{y}^p = f_{LM}(h^1,h^2,...,h^{N_V},...,h^p)$
其中 $h^1,...,h^{N_V}=C(f_V(X))$ 并且 $h^p = Emb(\hat{y}^{p-1})$对于 $p > N_V$,也就是说对于后续位置，输入是前一个预测标记 $\hat{y}^{p-1}$的嵌入表示

预测开始条件： $h^{N_V+1}$被定义为句子开始标记

输出token的生成过程：
* 对于最后一层位置 $p$的token $h^{N_L}_p$进行归一化
* 应用反潜入层，包含矩阵 $W_U$乘法和softmax操作
* 得到预测标记 $\hat{y}^p$

完整的预测描述 $\hat{y}$:
* 由位置 $p>N_V$的所有预测标记组成： $\hat{y} = \lbrace \hat{y}^p \rbrace_{p>N_V}$
* 预测过程持续到生成句子结束标记为止

详细的工作原理：
  1. 视觉部分
     * 图像 $X$输入到视觉编码器 $f_V$,生成视觉特征
     * 将视觉特征通过连接器 $C$转换为 $N_V$个视觉标记：$h_1,h_2,...,h_{N_V}$

2. 文本部分处理：
   * 位置 $p > N_V$的标记是模型自回归生成的文本部分
   * 每个文本位置的输入 $h_p$是前一个位置预测标记 $\hat{y}^{p-1}$的嵌入表示
   * 体现了模型的自回归特性，即当前预测依赖于之前的预测

预测过程的详细步骤：
1. 初始化预测：

   a.预测从位置 $N_V+1$开始，使用特殊的句子开始标记
     
   b. 此标记用于告诉模型开始生成描述文本

2. 自回归预测循环，对于每个位置 $p > N_V$:

     a. 模型处理当前的完整蓄力，这个序列包含所有的视觉标记和目前为止的所有文本标记
     
     b. 语言模型 $f_{LM}$通过多层处理这个序列
     
     c. 每一层 $l$的表示为 $h^p_{(l)}$共有 $N_L$层
     
3. 最终标记预测
     
     a. 取位置 $p$在最后一层 $N_L$的表示 $h^p_{(N_L)}$

     
     b. 对该表示进行归一化处理

     
     c. 应用反嵌入矩阵 $W_U$将其映射到词汇表空间

     
     d. 使用softmax函数将结果转换为词汇表上的概率分布

     
     e. 选择概率最高的token作为预测结果

     
4. 描述生成完成：

     a. 预测过程持续到模型生成句子结束标记

     
     b. 最终预测的描述 $\hat{y}$是位置 $p > N_V$的所有预测标记的集合

     
     c. 输出只包含文本部分

     
<img width="449" alt="image" src="https://github.com/user-attachments/assets/6974cee9-e009-43f5-adb2-587ea9a7b775" />

**Training**

研究聚集与“将图像翻译为文本的模型”，这些模型通常冻结视觉编码器 $f_v$，然后只训练连接器 $C$，这个连接器的作用是将视觉转换为语言模型可理解的表示；但是很多模型现在也开始使用微调LLM来提升性能，作者使用了两种方法，即只训练连接器或者同时微调语言模型，但是他认为LLM对多模态输入的泛化能力需要深入理解，因此只关注“保持LLM不变”的设置，这种设置已经可以看到明显的语言能力泛化为视觉-语言任务

### 2. Method overview

<img width="644" alt="image" src="https://github.com/user-attachments/assets/61dd9765-bd97-4e8a-9fa4-eae181a0fbef" />

上述为cox-lmm的一个示意图，以下为详细解释：

1. 选择相关图像并提取表示：
   
   * 从数据集 $S$中选择与目标token $t$相关的图像子集 $X$，这里的 $t$表示要选择的概念
     
   * 将这些图像输入与训练的多模态模型 $f$中进行处理
     
   * 提取维度为 $B$的表示，收集到矩阵 $Z$中， $Z\in \mathbb{R^{B\times M}}$，其中 $M$是样本数量


2. 线性分解表示矩阵
   
   * 将表示矩阵 $Z$近似分解为 $Z \approx UV$

   * $U$是大小为 $B\times K$的学习概念字典，包含 $K$个概念
  
   *  $V是大小为 $K\times M$的激活矩阵，表示每个样本对应概念的激活强度

3. 概念的多模态语义解释
   
   * 将学习到的概念字典 $U$中的多模态概念在视觉和文本两个模态中进行语义解释
  
### 3. Representation extraction

**样本选择**

* 从数据集 $S$中选择样本集 $X$,用于提取目标概念$t$的表示信息

* 选择条件有两个部分：

  *  $t$出现在模型预测的描述 $\hat{y}$中： $t \in f(X_i)$
 
  *   $t$也出现在真实标记描述 $y_i$中： $t \in y_i$

  这确保了所选择样本不仅与模型对标记 $t$的理解相关，也与真实的视觉对应关系有关

所以样本集合的形式化定义为：

  $X = \lbrace {X_i} | t\in f(X_i) and t \in y_i and (X_i,y_i) \in S$

**表示提取的具体方法**

提取位置为 $p > N_V$(也就是说文本部分)，且该位置预测的标记正好是目标标记 $\hat{y}^p = t$，并且通常情况下选择token $t$首次出现的位置：

example: 当前 $t$为‘dog’,输出预测为 ‘a dog sitting on a coach’，那么他会选择语义中的'dog'的位置上上进行特征提取

**表示特征的解释**

通常情况下会选在 $h^p_{(l)}$进行提取，因为这个时候已经融合了多层次的处理信息，得到的结果更加丰富并且全面

### 4. Decompsing the representations

使用矩阵分解，得到概念字典矩阵和激活矩阵：

$Z = \approx UV$

*  $U \in \mathbb{R}$^{B\times K}：概念字典矩阵，列向量是概念向量

*  $V \in \mathbb{R}$^{K\times M}: 激活矩阵，表示每个样本对各概念的激活程度

* 其中 $K$是字典元素数量

与其他的一些分解方法，类似于pca，k-means,nmf可以提供更加易于解释的结果，但是由于 $Z$矩阵含有负值，所以可以使用半非负矩阵分解(Semi-NMF)   

优化问题：

* 目标函数：最小化重构误差， $||Z-UV||_F+\lambda ||V||_1$,其中 $\lambda ||V||_1$鼓励V的稀性，表示任何样本只应该激活少量概念

因此得到的概念表示为：


<img width="579" alt="image" src="https://github.com/user-attachments/assets/630037fc-eda4-46bf-b226-fa8ea1ddc92b" />

 
新样本的概念激活计算：

* 使用学习到的概念字典 $U^*$

* 假设当前有一个新的图像 $X$,这个图像在模型预测中包含了我们感兴趣的token

* 相知道这个图像如何激活我们已经学习的概念

    * 对图像 $X$通过模型提取 $t$位置的内部表示 $Z_X$
 
    * 计算激活值：通过求解欧化问题计算概念激活值 $v(X)$

      $v(X) = argmin(v>=0)||Z_X-U^* * v|||^2_2+\lambda ||v||_1$

  * $v_k(X)$表示字典中第 $k$个概念 $u_k$对于图像 $X$的激活强度

### 5.Using the concept dictionary for interpretation

**Multimodal grouding of concepts：多模态接地**

1. 如何在视觉领域为学习到的概念提供具体解释(**视觉接地**：将周详的概念或表示与具体的视觉实例关联起来的过程，使得抽象概念能够在视觉领域中得到具体体现)

* 将数学上的概念向量转换为人类可以看到和理解的图像实例

（在这里使用的是最大激活）

**目标：** 为每个概念向量 $u_k$找到最能激活该概念的图像样本

**选择标准：** 通过查看概念激活值 $v_k(x)$来确定哪些图像最能代表该概念

$X_k,MAS$表示为概念 $u_k$选择的最大激活样本集合， $u_k$来自概念字典 $U$中，表示对于概念 $k$来说的概念向量；需要从图像集合 $X中选择 $N_{MAX}$个图像组成子集 $\hat{X}$:


  $X_{k，MAS} = argmax_{\hat{X}\subset X,|\hat{X}| = N_{MAX}}\sum_{X\in \hat{X}}|v_k(X)|$


也就是说，选择 $N_{MAX}$个图像，使得他们对概念 $u_k$的激活值绝对值之和最大

2. **文本接地**：如何在文本领域对概念向量进行语义解释

使用反嵌入层将概念向量 $u_k$映射回到词汇空间，反嵌入层通过矩阵 $W_U$将内部表示转换为词汇表上的概率分布，从这个分布中提取概率最高的token

<img width="632" alt="image" src="https://github.com/user-attachments/assets/2ae7efa4-bee5-4808-8b43-cd135fe705d4" />
[上图描述了为dog提取的某个概念的多模态接地]


**Most activating concepts for images**

用于找到对特定图像最活跃的概念，以理解LMM如何表示该图像

1. 分析过程

   * 对于给定图像 $X$,使用前面描述的方法提取其表示向量 $Z_X$
  
   * 将这个表示向量投影到已学习的概念字典 $U^*$上，得到激活向量 $V(X)$
  
   * $V(X)$的每个元素 $v_k(x)$ 表示对应于概念 $u_k$的激活强度

2. 最活跃概念的定义：

   * 定义 $\tilde{u}(X)$为对图像 $X$最活跃的概念集合
  
   * 这个集合包含 $r$个概念向量，他们在 $v(X)$中的激活值幅度最大，这些概念最能解释模型对图像 $X$的内部表示
  

## Experiments 

**实验模型**

* 主要使用DePALM模型，该模型在COCO数据集上训练用于图像描述任务
（DePALM模型：一类多模态模型设计思路，即通过专门设计的接口或者连接器，将与训练的单模态模型组合成多模态系统，Decomposed Prompt Alignment for Language Models）

* 视觉编码器：使用冻结的**ViT-L/14**提取图像特征
* 连接器：transformer结构，将视觉特征压缩为**10**个视觉标记，即 $N_V=10$
* 语言模型：使用冻结的**OPT-6.7B**生成文本
(OPT-6.7B:输入处理层(标记嵌入层+位置嵌入层)+多层transformer+输出层)


**实验参数**

* 概念数量：K=20
* 使用反嵌入层之前的最后一层
* Semi-NMF的正则化参数为 $\lambda=1$
* 视觉接地：每个概念选择5个最大激活样本
* 文本接地：每个概念选择15个最高概率token,然后过滤

**Overlap/entanglement of learnt concepts**

衡量学习到的概念之间的重叠或者纠缠程度，用于量化提取到的概念质量，用于评估概念字典中各个概念之间的关系

理想情况下：

* 字典 $U^*$中的每个概念 $u_k$应该编码关于不同概念 $t$的不同信息
* 不同概念 $u_k$和 $u_l(k\neq l)$应该于不同的词集相关联

概念重叠的量化方法：
* 通过计算接地词集合 $T_k$和 $T_l$之间的重叠来衡量
* 单个概念 $u_k$的重叠度定义为：它与其他所有概念共有词的平均比例

数学公式：

$Overlap(U^*) = \frac{1}{K}\sum_k Overlap(u_k), \quad Overlap(u_k) = \frac{1}{(K-1)}\sum^K_{l=1,l\neq k} \frac{|T_l \cap T_k|}{|T_k|}$ 

* 单个概念的重叠度：

    * 对每个其他概念 $u_l$，计算与 $u_k$共有的词占 $u_k$总词数的比例
    * 然后与所有 $K-1$个其他概念取平均

* 整个字典的重叠/纠缠度量：

    * 对所有$K个概念的重叠度取平均

较低的重叠度表示概念之间更加区分明确，每个概念捕捉独特的特征

**Multimodal grounding of concepts**

评估概念的多模态接地质量，用于验证提取到的概念是否真的具有多模态特性

* 评估概念 $u_k$的视觉接地 $(X_k,MAS)$和文本接地 $(T_k)$之间的一致性或对应程度
* 检验图像和文本是否描述了相同的概念

评估方法：

* CLIPScore:一种基于CLIP模型的跨模态相似度评分

    a. 对于每个概念 $u_k$收集其最大激活样本的激活，并且获得其接地词集合 $T_k$

    b. 将 $T_k$中的词组合成为文本描述

    c. 使用CLIP计算图像与文本之间的余弦相似度
  
* BERTScore:一种基于BERT模型的文本相似度评分

    a.对于每个概念 $u_k$最大激活的图像样本集合 $x_k,MAS$,获取这些图像的真实描述

    b. 将概念 $u_k$的接地词集合 $T_k$与这些真实描述进行比较

    c. 使用BERT模型计算接地词与真实描述之间的语义相似度

**Baseline**

1. 基于字典学习变体的基线

   * 使用不同的字典学习策略来实现Cox-LMM
  
       a. PCA

       b. KMeans

       c. Semi-NMF

   * 用于CLIPScore/BERTScore评估的基线：
  
      a. Rnd-words:随机词基线，用于评估概念的文本接地是否具有真正的语义相关性，排除任何词集都可能与视觉样本产生某种程度的相关性

         1. 使用semi-nmf作为基础学习方法

         2. 将每个概念 $u_k$的接地词 $T_k$替换为随机词集合 $R_k$

         3. 保持词集大小相同( $|R_k| = |T_k|$)

         4. 对随机采样的token进行编码，选择符合条件的顶部词

     b. Noise-Imgs(噪声图像)，使用随机噪声作为图像输入

     c. Simple:(使用最大范数，但是这没有一定的因果关系)

        不使用矩阵分解来学习概念，而是直接从数据中选择具有代表性的样本

         a. 从原始表示矩阵 $Z$中选择那些范数较大的token表示作为概念向量

         b. 这些高范数的表示被认为是最显著或包含最多信息的样本

         c. 类似一种最近邻分类方法，在transformer模型的较深层中，概念的表示已经充分融合了相关的语义、视觉和上下文信息，已经转换为清晰的概念表示；范数大的表示通常包含更强的信号和更显著的特征，在深层，高范数表示往往对应与模型强烈激活的概念
         

<img width="627" alt="image" src="https://github.com/user-attachments/assets/c8ec018d-10b8-476e-b8d3-633696c7b89c" />

上述为测试结果

###多义性测试 (appendix B)


分析了模型中学习到的"Dog"词元相关的概念向量，目的是了解这些向量是倾向于对特定语义概念强烈激活(单义性)还是对多个语义概念都有激活(多义性)

**experiment**

手动标注了 **160** 个dog测试样本，关注四个特定语义概念：

* ‘hot dog’
* 'black dog'
* 'brown/orange dog'
* 'bull dog'

对于每个语义概念，定义了真实集合 $C_{True}$

对应的概念向量 $u_k$，找出激活值大于阈值 $\tau$的测试样本集合 $C_{top}$(阈值设为测试样本中最大激活值的一半)

计算概念向量的特异性： $\frac{C_{top}\cap C_{True}}{C_{top}}$ (即 $C_{top}$中有多少比例的样本也在真实集合 $C_{True}$中)

**研究发现**

* ‘hot dog’最具有单义性，特异性达 100%
* ‘black dog’的特异性为 93.3%
* ‘bull dog’的特异性为 50%，这个概念也会对‘玩具狗/毛绒狗’的测试图像产生激活
* 'brown / orange dog'的特异性为76%，他也会对一些深色狗激活

**结论**

* 突出或明显的语义概念往往由更具单义性的概念向量捕捉
* 稀疏存在的概念更容易被叠加
* 神经网络中的概念并不总是一对一的映射，有些向量可能具有清晰特定的语义，而其他向量则可能捕捉多个相关的概念混合表示





















