# CoX-LMM -> 用于解释大型多模态模型的内部表示
借鉴CNN领域中的概念激活向量方法，开发专门适用于大型多模态模型的可解释性技术

**Cox-LMM**:
1. 为特定词汇/标记学习概念词典
2. 将模型内部表示通过字典学习方法进行线性分解
3. 分解得到的字典元素代表不同的概念

**关键发现**：
学习到的概念具有多模态语义结构，可以同时在视觉和文本领域中有意义地解释：
* 视觉上：通过提取最大激活这些概念的图像来体现
* 文本上：通过语言模型解码概念病提取相关词汇/标记来表达

**主要贡献**
1. 提出首个针对大规模多模态模型的概念解释框架
2. 开发**基于字典学习的概念提取方法**，使得概念可在文本和视觉上**同时解释**
3. 通过半非负矩阵分解优化改进了现有的概念字典学习策略
4. 通过定性可视化和定量评估验证了多模态概念的有效性

**Related-work**
1. 尽管视觉和感知标记不同，但是他们在大型语言模型内部被隐识对齐
2. ‘多模态神经元’：这些神经元对输入图像中的特定概念信息会被激活
3. 使用CLIP的视觉表示分解为文本表示来理解其内部机制
4. LMM中存在多模态神经元，这些神经元能将高级视觉信息转换为相应的文本信息

## Approach
### 1. Background for Large Multimodal Models
**Model Architecture**
考虑一个大型多模态模型 $f$ 的通用模型架构，他由一个可视编码器 $f_V$、一个可训练连接器 $C$，以及一个包含 $N_L$层的LLM $f_{LM}$。

当前模型针对图像描述任务进行了训练(captioning)，底层数据集 $S=\lbrace (X_i,y_i) \rbrace^N_{i=1}$ 由图像组成，其中图像 $X_i \in \mathcal{X}$并且与他们相关的标题 $y_i \in \mathcal{y}$,其中 $\mathcal{X}$和 $\mathcal{y}$分别表示图像空间和文本标记集。caption $y_i$可以被看作是所有tokens的子集，语言模型 $f_{LM}$的输入是有token $\lbrace h^1,h^2,...,h^p \rbrace$的序列表示，输出为 $\hat{y}$.

token在 $f_{LM}$的第 $l$层位置 $p$处的token表示为 $h^p_{(l)}$，其中 $h^p_{(0)}=h^p$表示位置 $p$处的初始输入表示

多模态模型的输入序列结构和预测过程如下：
多模态模型的语言模型 $f_{LM}$输入序列由两部分组成：
1. 视觉编码器 $f_V$处理图像 $X$后提供的 $N_V$个视觉标记，后接连接器 $C$
2. 语言模型之间预测的文本标记的线性嵌入


对于位置 $p > N_V$的情况，也就是非视觉标记的部分，预测过程表示为：

$\hat{y}^p = f_{LM}(h^1,h^2,...,h^{N_V},...,h^p)$
其中 $h^1,...,h^{N_V}=C(f_V(X))$ 并且 $h^p = Emb(\hat{y}^{p-1})$对于 $p > N_V$,也就是说对于后续位置，输入是前一个预测标记 $\hat{y}^{p-1}$的嵌入表示

预测开始条件： $h^{N_V+1}$被定义为句子开始标记

输出token的生成过程：
* 对于最后一层位置 $p$的token $h^{N_L}_p$进行归一化
* 应用反潜入层，包含矩阵 $W_U$乘法和softmax操作
* 得到预测标记 $\hat{y}^p$

完整的预测描述 $\hat{y}$:
* 由位置 $p>N_V$的所有预测标记组成： $\hat{y} = \lbrace \hat{y}^p \rbrace_{p>N_V}$
* 预测过程持续到生成句子结束标记为止

详细的工作原理：
  1. 视觉部分
     * 图像 $X$输入到视觉编码器 $f_V$,生成视觉特征
     * 将视觉特征通过连接器 $C$转换为 $N_V$个视觉标记：$h_1,h_2,...,h_{N_V}$

2. 文本部分处理：
   * 位置 $p > N_V$的标记是模型自回归生成的文本部分
   * 每个文本位置的输入 $h_p$是前一个位置预测标记 $\hat{y}^{p-1}$的嵌入表示
   * 体现了模型的自回归特性，即当前预测依赖于之前的预测

预测过程的详细步骤：
1. 初始化预测：

   a.预测从位置 $N_V+1$开始，使用特殊的句子开始标记
     
   b. 此标记用于告诉模型开始生成描述文本

2. 自回归预测循环，对于每个位置 $p > N_V$:

     a. 模型处理当前的完整蓄力，这个序列包含所有的视觉标记和目前为止的所有文本标记
     
     b. 语言模型 $f_{LM}$通过多层处理这个序列
     
     c. 每一层 $l$的表示为 $h^p_{(l)}$共有 $N_L$层
     
3. 最终标记预测
     
     a. 取位置 $p$在最后一层 $N_L$的表示 $h^p_{(N_L)}$

     
     b. 对该表示进行归一化处理

     
     c. 应用反嵌入矩阵 $W_U$将其映射到词汇表空间

     
     d. 使用softmax函数将结果转换为词汇表上的概率分布

     
     e. 选择概率最高的token作为预测结果

     
4. 描述生成完成：

     a. 预测过程持续到模型生成句子结束标记

     
     b. 最终预测的描述 $\hat{y}$是位置 $p > N_V$的所有预测标记的集合

     
     c. 输出只包含文本部分

     
<img width="449" alt="image" src="https://github.com/user-attachments/assets/6974cee9-e009-43f5-adb2-587ea9a7b775" />

**Training**

研究聚集与“将图像翻译为文本的模型”，这些模型通常冻结视觉编码器 $f_v$，然后只训练连接器 $C$，这个连接器的作用是将视觉转换为语言模型可理解的表示；但是很多模型现在也开始使用微调LLM来提升性能，作者使用了两种方法，即只训练连接器或者同时微调语言模型，但是他认为LLM对多模态输入的泛化能力需要深入理解，因此只关注“保持LLM不变”的设置，这种设置已经可以看到明显的语言能力泛化为视觉-语言任务

### 2. Method overview

<img width="644" alt="image" src="https://github.com/user-attachments/assets/61dd9765-bd97-4e8a-9fa4-eae181a0fbef" />

上述为cox-lmm的一个示意图，以下为详细解释：

1. 选择相关图像并提取表示：
   
   * 从数据集 $S$中选择与目标token $t$相关的图像子集 $X$，这里的 $t$表示要选择的概念
     
   * 将这些图像输入与训练的多模态模型 $f$中进行处理
     
   * 提取维度为 $B$的表示，收集到矩阵 $Z$中， $Z\in \mathbb{R^{B\times M}}$，其中 $M$是样本数量


2. 线性分解表示矩阵
   
   * 将表示矩阵 $Z$近似分解为 $Z \approx UV$

   * $U$是大小为 $B\times K$的学习概念字典，包含 $K$个概念
  
   *  $V是大小为 $K\times M$的激活矩阵，表示每个样本对应概念的激活强度

3. 概念的多模态语义解释
   
   * 将学习到的概念字典 $U$中的多模态概念在视觉和文本两个模态中进行语义解释
  
### 3. Representation extraction

**样本选择**

* 从数据集 $S$中选择样本集 $X$,用于提取目标概念$t$的表示信息

* 选择条件有两个部分：

  *  $t$出现在模型预测的描述 $\hat{y}$中： $t \in f(X_i)$
 
  *   $t$也出现在真实标记描述 $y_i$中： $t \in y_i$

  这确保了所选择样本不仅与模型对标记 $t$的理解相关，也与真实的视觉对应关系有关

所以样本集合的形式化定义为：

  $X = \lbrace {X_i} | t\in f(X_i) and t \in y_i and (X_i,y_i) \in S$

**表示提取的具体方法**

提取位置为 $p > N_V$(也就是说文本部分)，且该位置预测的标记正好是目标标记 $\hat{y}^p = t$，并且通常情况下选择token $t$首次出现的位置：

example: 当前 $t$为‘dog’,输出预测为 ‘a dog sitting on a coach’，那么他会选择语义中的'dog'的位置上上进行特征提取

**表示特征的解释**

通常情况下会选在 $h^p_{(l)}$进行提取，因为这个时候已经融合了多层次的处理信息，得到的结果更加丰富并且全面

### 4. Decompsing the representations

使用矩阵分解，得到概念字典矩阵和激活矩阵：

$Z = \approx UV$

*  $U \in \mathbb{R}$^{B\times K}：概念字典矩阵，列向量是概念向量

*  $V \in \mathbb{R}$^{K\times M}: 激活矩阵，表示每个样本对各概念的激活程度

* 其中 $K$是字典元素数量

与其他的一些分解方法，类似于pca，k-means,nmf可以提供更加易于解释的结果，但是由于 $Z$矩阵含有负值，所以可以使用半非负矩阵分解(Semi-NMF)   

优化问题：

* 目标函数：最小化重构误差， $||Z-UV||_F+\lambda ||V||_1$,其中 $\lambda ||V||_1$鼓励V的稀性，表示任何样本只应该激活少量概念

因此得到的概念表示为：


<img width="579" alt="image" src="https://github.com/user-attachments/assets/630037fc-eda4-46bf-b226-fa8ea1ddc92b" />

 
新样本的概念激活计算：

* 使用学习到的概念字典 $U^*$

* 假设当前有一个新的图像 $X$,这个图像在模型预测中包含了我们感兴趣的token

* 相知道这个图像如何激活我们已经学习的概念

    * 对图像 $X$通过模型提取 $t$位置的内部表示 $Z_X$
 
    * 计算激活值：通过求解欧化问题计算概念激活值 $v(X)$

      $v(X) = argmin(v>=0)||Z_X-U^* * v|||^2_2+\lambda ||v||_1$

  * $v_k(X)$表示字典中第 $k$个概念 $u_k$对于图像 $X$的激活强度

### 5.Using the concept dictionary for interpretation

**Multimodal grouding of concepts：多模态接地**

1. 如何在视觉领域为学习到的概念提供具体解释(**视觉接地**：将周详的概念或表示与具体的视觉实例关联起来的过程，使得抽象概念能够在视觉领域中得到具体体现)

* 将数学上的概念向量转换为人类可以看到和理解的图像实例

（在这里使用的是最大激活）

**目标：** 为每个概念向量 $u_k$找到最能激活该概念的图像样本

**选择标准：** 通过查看概念激活值 $v_k(x)$来确定哪些图像最能代表该概念

$X_k,MAS$表示为概念 $u_k$选择的最大激活样本集合， $u_k$来自概念字典 $U$中，表示对于概念 $k$来说的概念向量；需要从图像集合 $X中选择 $N_{MAX}$个图像组成子集 $\hat{X}$:


  $X_{k，MAS} = argmax_{\hat{X}\subset X,|\hat{X}| = N_{MAX}}\sum_{X\in \hat{X}}|v_k(X)|$


也就是说，选择 $N_{MAX}$个图像，使得他们对概念 $u_k$的激活值绝对值之和最大

2. **文本接地**：如何在文本领域对概念向量进行语义解释

使用反嵌入层将概念向量 $u_k$映射回到词汇空间，反嵌入层通过矩阵 $W_U$将内部表示转换为词汇表上的概率分布，从这个分布中提取概率最高的token

<img width="632" alt="image" src="https://github.com/user-attachments/assets/2ae7efa4-bee5-4808-8b43-cd135fe705d4" />
[上图描述了为dog提取的某个概念的多模态接地]


**Most activating concepts for images**

用于找到对特定图像最活跃的概念，以理解LMM如何表示该图像

1. 分析过程

   * 对于给定图像 $X$,使用前面描述的方法提取其表示向量 $Z_X$
  
   * 将这个表示向量投影到已学习的概念字典 $U^*$上，得到激活向量 $V(X)$
  
   * $V(X)$的每个元素 $v_k(x)$ 表示对应于概念 $u_k$的激活强度

2. 最活跃概念的定义：

   * 定义 $\tilde{u}(X)$为对图像 $X$最活跃的概念集合
  
   * 这个集合包含 $r$个概念向量，他们在 $v(X)$中的激活值幅度最大，这些概念最能解释模型对图像 $X$的内部表示 









