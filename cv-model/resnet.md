# Resnet详解
## 论文解析
关于深层模型的问题：
1. 梯度消失/爆炸：深层模型的主要障碍是梯度消失/爆炸，这使得模型很难收敛，但是通过标准化初始化和中间归一层可以基本解决
   关于梯度消失/爆炸：由于梯度计算的链式法则，梯度需要层层传递，当权重过大或者过小的时候，在多层梯度传递过程中会导致梯度呈现指数级放大或者缩小：
   * 如果权重 > 1，多层之后会出现梯度非常大 -> 进而出现爆炸的情况
   * 如果权重 < 1，多层之后会出现很小的梯度 -> 出现梯度消失
  使用标准化初始化如何帮助：
  * 标准初始化：均匀分布或者高斯分布没有考虑到网络结构
  * Xavier初始化：使得网络的前向和后向传播过程中保持方差一致 -> 针对对称性激活函数来说的：
    ```
    关于这个的推导过程：
      假设当前网络层为：y = wx,其中w~(0,Var(w)),x~(0,Var(x))
      由于w与x独立，并且假设当前有n_in个输入，那么对于y的方差的计算为：
        var(y) = n_in * var(w) * var(x)
    在前向传播过程中，为了使得方差相等，因此上述公式应该具有一下的特征：
        n_in * var(w) * var(x) = var(x)
        var(w) = 1/n_in
    同样，对于反向传播过程中：
      ∂L/∂x = ∂L/∂y * ∂y/∂x
            = ∂/∂y *w^T
      var(∂L/∂x) = n_out * var(w) * var(∂L/∂y)
    在后向传播过程中，为了使得方差相等，因此上述公式具有一下的特征：
        n_out * var(w) * var(∂L/∂y) = var(∂L/∂y)
    所以var(w) = 1/n_out
    同时考虑前后的两个约束，可以得到：
      var(w) = 2/(n_in + n_out)
    ```
  * He 初始化->专门为了relu函数设计的，考虑到relu会讲约一半的输入值置为0
    ```
    推导公式：
      y = relu(w*x) = max(0,w*x)
      由于relu将负值置为0，输出约一半的元素，为了保持方差，剩余的非零元素需要放大2倍，因此：
        var(w) = 2/n_in
    ```
  使用归一化层的作用：
  * 将输出进行归一化，使得其均值为0，方差为1；通过这种方法将输入分布进行重置，避免了分布偏移和梯度的问题
    
